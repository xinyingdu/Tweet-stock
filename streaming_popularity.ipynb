{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Importing Library and setting environment path\n",
    "import os\n",
    "import sys\n",
    "#set the path \n",
    "os.environ['SPARK_HOME'] = \"C:/spark-2.0.2-bin-hadoop2.7\"\n",
    "sys.path.append(\"C:/park-2.0.2-bin-hadoop2.7/bin\")\n",
    "sys.path.append(\"C:/spark-2.0.2-bin-hadoop2.7/python\")\n",
    "sys.path.append(\"C:/spark-2.0.2-bin-hadoop2.7/python/pyspark\")\n",
    "sys.path.append(\"C:/spark-2.0.2-bin-hadoop2.7/python/pyspark/lib\")\n",
    "sys.path.append(\"C:/spark-2.0.2-bin-hadoop2.7/python/pyspark/lib/pyspark.zip\")\n",
    "sys.path.append(\"C:/spark-2.0.2-bin-hadoop2.7/python/pyspark/lib/py4j-0.10.3-src.zip\")\n",
    "sys.path.append(\"C:/Program Files/Java/jre1.8.0_77/bin\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named py4j.java_gateway",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-625286fe221c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Streaming Program\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstreaming\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mStreamingContext\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSQLContext\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdesc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:/spark-2.0.2-bin-hadoop2.7/python\\pyspark\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconf\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrdd\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRDD\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfiles\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSparkFiles\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:/spark-2.0.2-bin-hadoop2.7/python\\pyspark\\context.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconf\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfiles\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSparkFiles\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_gateway\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlaunch_gateway\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mserializers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPickleSerializer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mUTF8Deserializer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[0mPairDeserializer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAutoBatchedSerializer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNoOpSerializer\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:/spark-2.0.2-bin-hadoop2.7/python\\pyspark\\java_gateway.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[0mxrange\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_gateway\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mjava_import\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mJavaGateway\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mGatewayClient\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_collections\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mListConverter\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named py4j.java_gateway"
     ]
    }
   ],
   "source": [
    "# Streaming Program\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ssc = StreamingContext(sc,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ssc.checkpoint( \"file:///Users/xinyingdu/Google Drive/UMN/Fall/MSBA 6330 Harvesting Big Data/MyProject/checkpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "socket_stream = ssc.socketTextStream(\"localhost\", 5555)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lines = socket_stream.window( 20 )\n",
    "# lines = socket_stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "fields = (\"tag\", \"count\" )\n",
    "fields2 = (\"time\", \"sentiment\" )\n",
    "Tweet = namedtuple( 'Tweet', fields )\n",
    "txt = namedtuple( 'txt', fields2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# (lines.map(lambda text: text.split( \"|\" ) )\\\n",
    "#   .map(lambda word: Tweet( word[0], word[1]))\n",
    "#   .foreachRDD(lambda rdd: rdd.toDF()\n",
    "#   .registerTempTable(\"sentiments\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(lines.flatMap( lambda text : text.split( \" \" ) )\n",
    "  .filter( lambda word: word.lower().startswith(\"@\") )\n",
    "  .map( lambda word: ( word.lower(), 1) )\n",
    "  .reduceByKey( lambda a, b: a + b )\n",
    "  .map( lambda rec: Tweet( rec[0], rec[1] ) )\n",
    "  .foreachRDD(lambda rdd: rdd.toDF()\n",
    "  .sort(desc(\"count\")).limit(10).registerTempTable(\"tweets\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sqlContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ssc.start()\n",
    "#ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tweet(tag=u'#GrabYourWalletMon Dec 12 02:52:21 +0000 2016', count=0.4404)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import plotly.plotly as py  \n",
    "import plotly.tools as tls   \n",
    "import plotly.graph_objs as go\n",
    "import plotly \n",
    "plotly.tools.set_credentials_file(username='xinyingdu', api_key='Ssz6FAoJ34JBnxEy7ltn')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "token_1 = 'taj2aa9og4'\n",
    "# token_2 = 'e7sfqqk6vf'\n",
    "stream_id1 = dict(token=token_1, maxpoints=1000)\n",
    "# stream_id2 = dict(token=token_2, maxpoints=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trace1 = go.Scatter(x=[], y=[], stream=stream_id1, name='Popularity')\n",
    "#trace2 = go.Scatter(x=[], y=[], stream=stream_id2, yaxis='y2', name='Sentiment', marker=dict(color='rgb(148, 103, 189)'))\n",
    "\n",
    "data = go.Data([trace1])\n",
    "layout = go.Layout(\n",
    "    title='Ebay Twitter Stream',\n",
    "    yaxis=dict(\n",
    "        title= 'Popularity'))\n",
    "#     ),\n",
    "#     yaxis2=dict(\n",
    "#         title='Sentiment',\n",
    "#         titlefont=dict(\n",
    "#             color='rgb(148, 103, 189)'\n",
    "#         ),\n",
    "#         tickfont=dict(\n",
    "#             color='rgb(148, 103, 189)'\n",
    "#         ),\n",
    "#         overlaying='y',\n",
    "#         side='right'\n",
    "#     )\n",
    "#)\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "\n",
    "# Send fig to Plotly, initialize streaming plot, open new tab\n",
    "py.iplot(fig, filename='ebay-twitter-streaming')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s_1 = py.Stream(stream_id=token_1)\n",
    "#s_2 = py.Stream(stream_id=token_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s_1.open()\n",
    "#s_2.open()\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "#count = 0\n",
    "while True:\n",
    "    time.sleep( 6 )\n",
    "    # getting twitter count \n",
    "    pop_count = sqlContext.sql( \"select sum(count) as pop from tweets WHERE tag LIKE '%ebay%'\")\n",
    "    pop_count_df = pop_count.toPandas()\n",
    "    x = datetime.datetime.now()\n",
    "    y = pop_count_df['pop'][0]\n",
    "    \n",
    "    # getting sentiments\n",
    "#     senti = sqlContext.sql( \"select mean(count) as senti from sentiments\")\n",
    "#     senti_df = senti.toPandas()\n",
    "#     x1 = datetime.datetime.now()\n",
    "#     y1 = senti_df.senti[0]\n",
    "        # Send data to your plot\n",
    "    s_1.write(dict(x=x, y=y))\n",
    "#     s_2.write(dict(x=x1, y=y1)) \n",
    "s_1.close()\n",
    "#s_2.close()\n",
    "#count = count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ssc.stop(False)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:gl-env]",
   "language": "python",
   "name": "conda-env-gl-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
